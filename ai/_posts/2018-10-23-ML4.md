# **ANN 정리**

## **Forward propagation: Vectorized implementation**

![image](https://user-images.githubusercontent.com/37058233/103744613-6e62d800-5041-11eb-951d-3e5004bb6c62.png)

이렇게 행렬로 한번에 나타내 줄 수 있다.

세타 행렬 기준으로

**행의 수:** 그 다음 layer의 Perceptron의 수. 

**열의 수:** 그전의 layer의 perceptron의 수. 

이 행렬을 다 곱하면, 결과물로 그 다음 layer의 perceptron의 값이 행렬로 나타난다. 

실제로는 아래처럼 bias 도 layer 마다 계속 추가된다. 

<img src="https://user-images.githubusercontent.com/37058233/103744665-8175a800-5041-11eb-8e66-49f78c966f31.png" width =300px>

 

 

## **Multiclass output unit**

맨 마지막 레이어에서 여러가지 숫자가 행렬로 나타난다. 이 숫자는, 0 에서 1 사이의 값을 가지게 된다. 가장 숫자가 큰 분류가 input의 분류라고 생각될 수 있다.

## **Cost function**

<img src="https://user-images.githubusercontent.com/37058233/103744698-8dfa0080-5041-11eb-8d4a-84ab2a797000.png" width=400px>

잘라서 보기

<img src="https://user-images.githubusercontent.com/37058233/103744719-96ead200-5041-11eb-8675-facc2fedaf52.png" width =400px>

예를 들어 클래스가 3개가 있다면, 우리의 예측모델은 클래스 3개에 대한 값을 내놓을 것이다. 그 값은 0과 1 사이다. 정답은 한 클래스만 1 이다. 

클래스가 3개 있다는 말은, 우리의 가설이 세개가 있다는 뜻이다. 

위의 식은, 같은 x,y를 이용해, 다른 세개의 가설에 대해 결과가 얼마나 맞는지를 보게 된다.

따라서 위와 같은 식이 얻어진다.  

<img src="https://user-images.githubusercontent.com/37058233/103744780-b124b000-5041-11eb-883e-169cdaa8fb6a.png" width=300px>

모든 세타를 제곱해서 더한후, 람다로 오버피팅되지 않게 조절해주고, 2m 으로 나누어주어서, 평균을 낸다. 

## **BACK PROPAGATION**

Forward propagation 을 통해서 total cost 값이 정해지면, 이 값을 존재하는 모든 세타에 대하여 각각 미분해서 그 값에다 learning rate 를 곱해서 빼준다. 

<img src="https://user-images.githubusercontent.com/37058233/103744797-baae1800-5041-11eb-9374-fa552d18729b.png" width =300px>

위 식에 대해선, 다음 정리 노트에 증명하도록 한다. 위는 전체 cost 에 대해 각각의 세타값을 미분해 준 것이다. 

<img src="https://user-images.githubusercontent.com/37058233/103744823-c4d01680-5041-11eb-86dc-4bc5c9ca50a4.png" width=200>

위는, gradient 를 update 한 것이다 .여기서 learing rate 는 1로 설정되어있고, 이것은 델타항에 곱해진다. 

## **Gradient checking**

<img src="https://user-images.githubusercontent.com/37058233/103744844-cef21500-5041-11eb-8c6a-02cfd208a791.png" width = 300>

입실론을 아주 작은 값으로 보낸 후, 우리가 미분한 것이 올바른지 체크해 준다. 

## **Random initialization**

세타값이 초기화 할 때 다 같다면, 그 다음 layer의 노드들의 숫자가 다 똑같게 나온다. 그러므로, 초기화는 random한 값으로 해주어야한다.