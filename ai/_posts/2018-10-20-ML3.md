# **Multi-class Classification and Regularization** 

## **Multiclass classification:**

두개가 아닌, 여러 개의 클래스로 분류하는것이다. 

<img src="https://user-images.githubusercontent.com/37058233/103743175-2ba00080-503f-11eb-84e9-ea5306d1d526.png" width=400px>

이런 경우에는 어떻게 분류를 해야 맞는가?

**One-vs-all**

<img src="https://user-images.githubusercontent.com/37058233/103743216-3b1f4980-503f-11eb-8807-1a73edac526d.png" width=400px>

찾고 싶은값, 나머지 값 이렇게 두개로 나누어서 h(x)를 찾는다. 

<img src="https://user-images.githubusercontent.com/37058233/103743256-45414800-503f-11eb-858e-4e078553e3a5.png" width=300px>

여기서 , 이 hypothesis 식이 가지는 의미를봤을때, 정해진 세타에서, 들어온 x가 y= i 일 확률이다. 

<img src="https://user-images.githubusercontent.com/37058233/103743293-4ffbdd00-503f-11eb-85b9-3058be39e328.png" width = 400px>

이렇게, hθ1 (x), hθ2 (x), hθ3 (x) 세 개가 나온다. 

X= a 라는 case가 들어왔다고 하자. 그럼, 이 a는 어느 분류에 속하는지 어떻게 알아볼까?

이는 위에 나온 개념을 통해 알 수 있다. 

hθ1 (a):  a가 1인 클래스에 속할 확률

hθ2 (a):  a가 2인 클래스에 속할 확률

hθ3 (a):  a가 3인 클래스에 속할 확률

 

이렇게, 세 함수가 확률을 의미한다. 따라서, 가장 큰 확률을 가지는 함수의 클래스가 입력 값의 클래스로 예측되는 것이다. 

 

**과적합을 피하기 위해서?**

\1.   Feature 을 줄인다.

\2.   Regularization 모든 feature을 남기지만, 세타가 과적합을 피할 수 있게 처리해준다. 

**Occam’s razer:**

어떤 사건을 설명할 때, 두가지 방법이 있다면, 둘 중 간단한게 보통 낫다.

 

## **Regularization**

Overfitting problem을 해결하기 위한 방법이다. Features 가 많아지면, training set에 overfitting 되기 때문에, generalization 이 어려워진다. 따라서, 세타의 값이나 크기를 줄인다.

 

Regularization을 한 cost function 이다.

<img src="https://user-images.githubusercontent.com/37058233/103743360-64d87080-503f-11eb-9d58-63c7f71fe454.png" width = 400px>

 

뒤에 람다*세타를 해주는 이유는 무엇인가?

<img src="https://user-images.githubusercontent.com/37058233/103743385-6dc94200-503f-11eb-8fde-04fc5ead06c9.png" width = 200px>

이렇게 overfitting 된 그래프가 있다. 이 그래프는 

<img src="https://user-images.githubusercontent.com/37058233/103743448-820d3f00-503f-11eb-8993-2b726de1eae8.png" width=400px>

이러한 cost function 에 의해서 만들어 졌을 것이고, 이 cost function의 값이 0에 가장 가까워지는 θ 를 정할 것이다. 바로 여기서 과적합이 일어나게 된다!

따라서, <img src="https://user-images.githubusercontent.com/37058233/103743468-8c2f3d80-503f-11eb-9ed1-29e375f3f9c0.png" width=70px>이 항목을 더해준다. 람다가 크면, 뒤 항목들이 작아야지만 cost function의 크기를 줄일 수 있다. 이렇게, 세타의 크기를 줄임으로써, 너무 많은 변수의 세타가 너무 큰 영향을 주지 못하도록 막는다. 따라서, 과적합을 피할 수 있다.

<img src="https://user-images.githubusercontent.com/37058233/103743486-96e9d280-503f-11eb-8f56-95d122219410.png" width=500px>

이렇게 미분을 해주어서, 알파를 곱해주어 기존 세타에서 빼주면 아래의 결과식이 나온다. 

J는 0부터 n까지 가능하다.

 

총 n+1 개의 세타가 업데이트 될 때 얼만큼 바뀌어 어떤 값을 가져야하는지를 나타낸다. 

<img src="https://user-images.githubusercontent.com/37058233/103743517-a406c180-503f-11eb-9ec2-befb4d6cbcda.png" width =350px>

 

이것은, 로지스틱 리그레션의 회귀이다. 

<img src ="https://user-images.githubusercontent.com/37058233/103743536-acf79300-503f-11eb-97bd-7b96fccc251b.png" width = 400px>

<img src="https://user-images.githubusercontent.com/37058233/103743562-b7b22800-503f-11eb-92a2-df6731396d79.png" width=400px>

저번 단원에서, linear 와 logistic 을 미분하면 같은 형식이 나타난다는 것을 살펴보았다. 따라서 , gradient descendant가 같다. 

<img src="https://user-images.githubusercontent.com/37058233/103743583-c00a6300-503f-11eb-836a-de86c3337667.png" width=400px>