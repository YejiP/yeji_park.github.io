# **Unsupervised learning**

## **Clustering**

<img src="https://user-images.githubusercontent.com/37058233/103746030-c8fd3380-5043-11eb-9f22-cd0d17875fb5.png" width=300>

\- Label 이 없는 데이터를 학습한다. 

\- feature 들을 가지고, 숨은 구조를 찾기 위해 사용한다. 

\- unsupervised learning 자체가 하나의 목적이 될 수 있다. (뉴스 비슷한 주제로 묶어주거나 쇼핑)

\- 다음 단계를 위한 전처리 (supervised learning 을 위해 label 만들어 줄 수 있다.)

\- clustering 자체가 label이 될 수 있다. 

 

## **K-means clustering**

<img src="https://user-images.githubusercontent.com/37058233/103746070-d61a2280-5043-11eb-8790-83dfb400c776.png" width=300>



\- k개의 군집으로 나누려고 한다. K는 우리가 임의로 정해준다. 

\- centroid 는, 군집의 중심이 되는 값이다. k개가 있다. 

\- centroid 를 먼저 잡을 때는, 여러가지 방법이 있다. Centroid를 모두 같은 값으로 초기화 시키는 것은 좋은 방법이 아니다. 

\- 자료 값에서 랜덤하게 선택하는 방법도 있다.

\- normalization 해주어야 한다. (유클리드 거리 쓰니까.. 한 변수가 너무 커버릴 때, 다른 변수의 영향력이 축소되어 나타난다.)

\-  z normalization :

(x-mean(x))/sigma

\- min max normalization:

(x- xmin)/ (x max – x min)



**과정**

<img src="https://user-images.githubusercontent.com/37058233/103746107-e3371180-5043-11eb-99de-fa14cb47fe32.png" width =200>

\1. Random 한 값으로 centroid 를 잡아준다.

\2. 각각의 자료 값이 어떤 centroid 에 가까운지 그룹 지어준다. 

\3. 그룹끼리 mean 좌표를 다시 잡아준다. 

\4. 이 mean 값들과 자료 값들과의 거리를 다시 측정해서, 다시 그룹지어준다.

\5. mean 값들이 변하지 않을 때까지 반복해준다. 



## **Calinski-Harabasz index**

<img src="https://user-images.githubusercontent.com/37058233/103746137-f0ec9700-5043-11eb-842e-d8352dbceda7.png" width=200>

**WSS(k)?**

= 각각의 cluster에 속한 자료가 그 cluster의 중심과 얼마나 가까운지 보여주는 지표이다.

예를들어 클러스터가, 5개 있다고 하자. 

첫번째 클러스터를 생각해보자. 이 첫번째 클러스터에 속하는 값들이 있을 것이다. 이것들의 mean 을 구해서, 이 mean 과의 거리가 얼마나 떨어져있는지 유클리디안 거리를 이용해서 구한다. 그리고 이것을 더해준다. 

이렇게, 모든 클러스터에 대해 구해주고 그 값을 더해준다.

**BSS(K)?**

= TSS(K) – WSS(K)

Cluster 간 거리를 의미한다. 

**TSS(k)?**

Cluster 의 개수와 상관없이 항상 값이 같다. 

모든 자료의 평균 값과 모든 자료가 얼마나 떨어져있는지를 본다. 

**CH index**

크다는 의미는, bss 값이 크고, wss 가 작다는 것이니까, clustering 이 잘되었다는 것을 의미한다. 

## **Hierachical clustering**

 

<img src="https://user-images.githubusercontent.com/37058233/103746157-fba72c00-5043-11eb-9037-7fbccdf8a42a.png" width=500>

 

각각 자료 사이의 거리를 먼저 구해 놓는다.

그리고 가까운 것 끼리 묶는다. 계속 그런다. 

따라서, 모든 k에 대해서 이미 다 구해놓았다. 

직관적으로 볼 수 있다.  

 