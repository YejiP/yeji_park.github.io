# **Linear regression** **정리**

**Training set 에서의 용어**

```
m : 행의 수 (예시의 수)

x : input, features (여러 개) , ind (independent)

y : output, dep(dependent)
```

## **선형회귀의 가정**

**x와 y 사이에는 선형 관계가 있다.** (실제론, 있을 수도 있고 없을 수도 있다.) 

수식으로 표현하면, h (hypothesis)

h(x)=Θ2 + Θ1x

x와 h(x)의 관계가 있다고 생각함. 그걸 식으로 나타난 것이 위다. 

Θ1, Θ2는 어떻게 구할까? : **cost function**을 이용한다.!

**Cost function 이란?**

![image](https://user-images.githubusercontent.com/37058233/102959860-ffa35c00-4523-11eb-84c0-ecdfa86be15f.png)

H(x(i)) 이 부분은, 선형식을 가설로 세워, 그 값에다가 여러가지 관측 값을 넣어서, 예측한 값이다. Y(i) 는, 실제의 값이다.

이 두개의 차이는, 예측 값과 실제값 사이의 오차를 나타낸다. 이것을 제곱을 해주어서 더해줌으로, 플러스 마이너스가 상쇄되지 않고, 모든 케이스에서 총 오차를 알 수 있다. 그 후, 그 오차의 평균을 구한다. 따라서, 이 함수의 최솟값이 오차를 최소로 하는 지점이고, 이 오차를 최소로 하는 세타 값을 통해서 h(x)를 정한다. 

![image](https://user-images.githubusercontent.com/37058233/102959880-12b62c00-4524-11eb-8154-2a147c9bae9b.png)

풀어서 쓰면 이렇게 된다. 

, MSE 가 가장 작은 아이가 RMSE도 가장 작다. 

 

## **Gradient Descendant**

![image](https://user-images.githubusercontent.com/37058233/102959907-22ce0b80-4524-11eb-88c3-d16719da94e2.png)

특정한 Θ0을 정한다. Θ1을 다르게 하면서, J(Θ1) 을 구한다. 그게 아래 그래프이다. 

아래 그래프에서, 기울기를 구한 다음에, 기울기의 반대인 방향으로 간다. 

어느 정도 가야 하는지는 어떻게 정하는가?

![image](https://user-images.githubusercontent.com/37058233/102959971-3aa58f80-4524-11eb-8c72-085daf3f6ba4.png)

가는 비율을 learning rate 라고 한다. Learning rate가 너무 크거나 작으면 효율 상 좋지 않다. Learning rate는 보통 0.1, 0.05, 0.001 등을 사용한다. 

왼쪽처럼, 너무 적은 learning rate를 사용하면, 최저점으로 가는데 시간이 너무 오래 걸리고, 큰 learning rate 를 사용하면, 최저점을 보지 못하고 지나칠 경우가 있다. 

<img src ="https://user-images.githubusercontent.com/37058233/102960074-80625800-4524-11eb-8aae-967cdf463d73.png" width = 200px>

Non convex 함수이다. Local minimum 과 global minimum 이 다르다



<img src = "https://user-images.githubusercontent.com/37058233/102960219-cb7c6b00-4524-11eb-972c-c0450ab3eab6.png" width = 400px>

Convex 함수이다. Local minimum과 global minimum 이 하나다. 우리가 다룰 함수는 이것이다

<img src ="https://user-images.githubusercontent.com/37058233/102960316-0a122580-4525-11eb-96fd-221a5ad44da5.png" width = 400px>

<img src="https://user-images.githubusercontent.com/37058233/102960791-0fbc3b00-4526-11eb-9f1b-fbf7dcd386ff.png" width = 600px>

위같이 이 평면으로 잘랐을 때, 이러한 형태가 나타난다. 평면으로 자른다의 의미는, 세타1의 값이 정해진것이다. 위의 예시에는 세타 1의 값은 -10인경우이다



![image](https://user-images.githubusercontent.com/37058233/102960866-3f6b4300-4526-11eb-9628-0344da451003.png)

위의 별 지접에서, 어떻게 가야하는가를 정하기 위해서 아래의 왼쪽 방법을 사용한다.

![image](https://user-images.githubusercontent.com/37058233/102960917-645fb600-4526-11eb-856c-c9c1d9a09b6e.png)

위 그래프의 별 지점을 (Θ0, Θ1)라고 할 때, 저 지점에서 얼마나 움직여야하는가에 대한 문제는 아래의 식을 이용해서 알게 된다. x,y 따로 본다. Θ0 에서 learning rate * 그 지점에서의 Θ0의 기울기를 빼준다! ( 빼주는 이유는, 그 기울기에 반대되는 쪽으로 가야지 0에 가까운 곳을 찾을 수 있기 때문에.)

Temp0 := Θ0 – α*{Θ0에 대해 미분(J(Θ0, Θ1))}

Temp1 := Θ1 – α*{Θ1에 대해 미분(J(Θ0, Θ1))}

![image](https://user-images.githubusercontent.com/37058233/102962073-6414ea00-4529-11eb-97ed-06fa69075eec.png)

이렇게 하면 틀리다. 왜냐면, 세타0을 먼저 업데이트하면, 위치가 달라지므로 세타1의 기울기도 달라지게 된다.

![image](https://user-images.githubusercontent.com/37058233/102962151-81e24f00-4529-11eb-8da3-bc747dc6c725.png)

## **Linear regression with multiple variables**

![image](https://user-images.githubusercontent.com/37058233/102962190-9f171d80-4529-11eb-9c73-1538a592910c.png)

변수가 여러 개다.

X0은 1 로 정의한다. 

변수들 끼리, scale 이 다른 것을 감안하여, mean nomalization 을 해준다. 

그 이후는, 그냥 변수가 여러 개 추가된것과 같다.